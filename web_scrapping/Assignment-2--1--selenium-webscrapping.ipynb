{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46db34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required packages to do webscrapping using selenium \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import selenium\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e171ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cff29c46",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the webdriver chromdriver, it will open the chrome brower\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# open naukri browser and maximize it\n",
    "driver.get('https://www.naukri.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "# find the  Skill, Designations class name  and pass the Data analyst  as value\n",
    "destination=driver.find_element(By.CLASS_NAME,'suggestor-input ')\n",
    "destination.send_keys('Data Analyst')\n",
    "\n",
    "# find the location xpath and pass the bangalore key\n",
    "city=driver.find_element(By.XPATH,\"/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input\")\n",
    "city.send_keys('Bangalore')\n",
    "\n",
    "\n",
    "# search the job using search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "\n",
    "\n",
    "# scrape the job-title, job-location, company_name, experience_required from top 10 jobs\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "\n",
    "job=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "\n",
    "for i in job[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "location=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "\n",
    "for i in location[0:10]:\n",
    "    area=i.text\n",
    "    job_location.append(area)\n",
    " # extrct the company name       \n",
    "c_name=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "for i in c_name[0:10]:\n",
    "    company_name.append(i.text)\n",
    " # extrct the experience    \n",
    "exp=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "\n",
    "for i in exp[0:10]:\n",
    "    experience_required.append(i.text)\n",
    "\n",
    "# create the data frame for 10 values   \n",
    "df=pd.DataFrame({'job_title':job_title,'job_location':job_location,\n",
    "                'company_name':company_name,'experience_required':experience_required})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8ace6",
   "metadata": {},
   "source": [
    "2:Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the \n",
    "location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results youget.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the webdriver chromdriver, it will open the chrome brower\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# open naukri browser and maximize it\n",
    "driver.get('https://www.naukri.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "# find the  Skill, Designations class name  and pass the Data analyst  as value\n",
    "destination=driver.find_element(By.CLASS_NAME,'suggestor-input ')\n",
    "destination.send_keys('Data Scientist')\n",
    "\n",
    "\n",
    "# find the location xpath and pass the bangalore key\n",
    "city=driver.find_element(By.XPATH,\"/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input\")\n",
    "city.send_keys('Bangalore')\n",
    "\n",
    "\n",
    "# search the job using search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scrape the job-title, job-location, company_name, experience_required from top 10 jobs\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "\n",
    "\n",
    "job=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "\n",
    "for i in job[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "\n",
    "location=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "\n",
    "for i in location[0:10]:\n",
    "    area=i.text.split(\",\")[0]\n",
    "    l=area.split(\"/\")[0]\n",
    "    job_location.append(l)\n",
    "    \n",
    " # extrct the company name       \n",
    "c_name=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "for i in c_name[0:10]:\n",
    "    company_name.append(i.text)\n",
    " # extrct the experience    \n",
    "exp=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "\n",
    "\n",
    "\n",
    "# create the data frame for 10 values   \n",
    "df=pd.DataFrame({'job_title':job_title,'job_location':job_location,\n",
    "                'company_name':company_name})\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381ac43",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results. \n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get thewebpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results youg    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the webdriver chromdriver, it will open the chrome brower\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# open naukri browser and maximize it\n",
    "driver.get('https://www.naukri.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "# find the  Skill, Designations class name  and pass the Data analyst  as value\n",
    "destination=driver.find_element(By.CLASS_NAME,'suggestor-input ')\n",
    "destination.send_keys('Data Scientist')\n",
    "\n",
    "\n",
    "# search the job using search button\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter on Delhi\n",
    "city=driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[4]/div/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]')\n",
    "city.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter on salar range 3-6” lakhs\n",
    "sal=driver.find_element(By.XPATH,'//*[@id=\"root\"]/div[4]/div/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]')\n",
    "sal.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list store the values\n",
    "job_title=[]\n",
    "job_location=[] \n",
    "company_name=[]\n",
    "experience=[]\n",
    "# collect elements from class tag, only 10\n",
    "title=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "location=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "c_name=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "exp=driver.find_elements(By.XPATH,'//li[@class=\"fleft br2 placeHolderLi experience\"]')\n",
    "\n",
    "# extract top 10 job-title,location,company-name,experience \n",
    "for i in title[0:10]:\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "for j in location[0:10]:\n",
    "    job_location.append(j.text)\n",
    "    \n",
    "for k in c_name[0:10]:\n",
    "    company_name.append(k.text)\n",
    "    \n",
    "for l in exp[0:10]:\n",
    "    experience.append(l.text)\n",
    "\n",
    "    #create the dataframe\n",
    "df=pd.DataFrame({'job_title':job_title,'job_location':job_location,'company_name':company_name,'experience':experience})\n",
    "df\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e419186",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url :https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and \n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the \n",
    "required data asusual\n",
    "\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page asusual\n",
    "6. Repeat this until you get data for 100sunglasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the webdriver chromdriver, it will open the chrome brower\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "# open filpkart website\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(5)\n",
    "#maximize the screen\n",
    "driver.maximize_window()\n",
    "# to avoid the login pop\n",
    "import pyautogui\n",
    "pyautogui.press('esc')\n",
    "\n",
    "# search for the sunglasses\n",
    "search=driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]')\n",
    "search.send_keys('sunglasses')\n",
    "# click on the search tab\n",
    "buton=driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]')\n",
    "buton.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1152e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create empty lists\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "# predefine the page numbers\n",
    "start=0\n",
    "end=3\n",
    "\n",
    "# start from first stage to 3 page\n",
    "for i in range(start,end):\n",
    "    # get the brand name \n",
    "    brand_name=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    # get the product title \n",
    "    title=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    # get the price\n",
    "    pr=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    \n",
    "    for i in brand_name:\n",
    "        brand.append(i.text)\n",
    "    for i in title:\n",
    "        description.append(i.text)\n",
    "    for i in pr:\n",
    "        price.append(i.text)        \n",
    "# click the next page button to go to next page\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]') \n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "print(len(brand))  \n",
    "print(len(description))  \n",
    "print(len(price))  \n",
    "\n",
    "#create the data frame\n",
    "df=pd.DataFrame({'brand':brand[0:100],'description':description[0:100],'price':price[0:100]})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d616b9",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product\u0002reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market \n",
    "place=FLIPKAR\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d672cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "# open filpkart website\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(5)\n",
    "#maximize the screen\n",
    "driver.maximize_window()\n",
    "# to avoid the login pop\n",
    "import pyautogui\n",
    "pyautogui.press('esc')\n",
    "\n",
    "# search for the sunglasses\n",
    "search=driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]')\n",
    "search.send_keys('iphone11')\n",
    "# click on the search tab\n",
    "buton=driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]')\n",
    "buton.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# open the iphone11 page to extract the review\n",
    "phone_page=driver.find_element(By.XPATH,'//a[@class=\"_1fQZEK\"]')\n",
    "link=phone_page.get_attribute('href')\n",
    "driver.get(link)\n",
    "# click on all review\n",
    "all_review=driver.find_element(By.XPATH,'//div[@class=\"_3UAT2v _16PBlm\"]')\n",
    "all_review.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111751cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=10\n",
    "Rating=[]\n",
    "Review_summary=[]\n",
    "full_Review=[]\n",
    "\n",
    "\n",
    "for i in range(start,end):\n",
    "    # collect all the reviews,ratings and full review\n",
    "    Rating_No=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for j in Rating_No[0:100]:\n",
    "        Rating.append(j.text)\n",
    "    review=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for k in review[0:100]:\n",
    "        Review_summary.append(k.text)\n",
    "    review_desc=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for l in review_desc[0:100]:\n",
    "        full_Review.append(l.text)\n",
    "    # click the next page button to go to next page\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]') \n",
    "    next_button.click()\n",
    "    time.sleep(3)    \n",
    "    \n",
    "        \n",
    "print(len(Rating))\n",
    "print(len(Review_summary))\n",
    "print(len(full_Review))\n",
    "\n",
    "  # create the the dataframe  \n",
    "df=pd.DataFrame({'Rating':Rating,'Review_summary':Review_summary,'full_Review':full_Review})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379e5e3",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the \n",
    "search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "# open filpkart website\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(5)\n",
    "#maximize the screen\n",
    "driver.maximize_window()\n",
    "# to avoid the login pop\n",
    "import pyautogui\n",
    "pyautogui.press('esc')\n",
    "\n",
    "# search for the sunglasses\n",
    "search=driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]')\n",
    "search.send_keys('sneaker')\n",
    "# click on the search tab\n",
    "buton=driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]')\n",
    "buton.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55886b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first extract the product link to open them in new tab\n",
    "start=0\n",
    "end=3\n",
    "link=[]\n",
    "for i in range(start,end):\n",
    "    ln=driver.find_elements(By.XPATH,'//a[@class=\"_2UzuFa\"]')\n",
    "    for i in ln:\n",
    "        page=i.get_attribute('href')\n",
    "        link.append(page)\n",
    "        \n",
    "    \n",
    "    # click the next page button to go to next page\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]') \n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "len(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be3fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open page 1 by 1 to scrap the details\n",
    "Brand_name=[]\n",
    "ProductDescription=[]\n",
    "price=[]\n",
    "for i in link[0:100]:\n",
    "    driver.get(i)\n",
    "    brand=driver.find_element(By.XPATH,'//span[@class=\"G6XhRU\"]')\n",
    "    Brand_name.append(brand.text)\n",
    "    \n",
    "    #expanding the product details page     \n",
    "    open_Desc=driver.find_element(By.XPATH,'//div[@class=\"col col-1-12 _2jufoV\"]')\n",
    "    open_Desc.click()\n",
    "# i have tried to to get all summary also but in many of the product there is no product summary inspite of that, code is getting break hence i am removing it\n",
    "    #read_more=driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _1zH-yM\"]')\n",
    "    #read_more.click()\n",
    "    # extract the product details\n",
    "    product_details=driver.find_element(By.XPATH,'//div[@class=\"row _1v8OXw\"]')\n",
    "    ProductDescription.append(product_details.text)\n",
    "    #extract the product price\n",
    "    pr=driver.find_element(By.XPATH,'//div[@class=\"_30jeq3 _16Jk6d\"]')\n",
    "    price.append(pr.text)\n",
    "    \n",
    "#create the dataframe \n",
    "df=pd.DataFrame({'Brand':Brand_name,'ProductDescription':ProductDescription,'price':price})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d49972",
   "metadata": {},
   "source": [
    "Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then \n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "# open filpkart website\n",
    "driver.get('https://www.amazon.in/')\n",
    "time.sleep(5)\n",
    "#maximize the screen\n",
    "driver.maximize_window()\n",
    "# to avoid the login pop\n",
    "import pyautogui\n",
    "pyautogui.press('esc')\n",
    "\n",
    "# search for the sunglasses\n",
    "search=driver.find_element(By.XPATH,'//input[@class=\"nav-input nav-progressive-attribute\"]')\n",
    "#search.click()\n",
    "search.send_keys('Laptop')\n",
    "# click on the search tab\n",
    "buton=driver.find_element(By.XPATH,'//div[@class=\"nav-search-submit nav-sprite\"]')\n",
    "buton.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the “Intel Core i7” laptops\n",
    "cpu_type=driver.find_element(By.XPATH,'//a[@class=\"a-link-normal s-navigation-item\"]')\n",
    "lap=cpu_type.get_attribute('href')\n",
    "driver.get(lap)\n",
    "\n",
    "Title=[]\n",
    "Ratings=[]\n",
    "Price=[]\n",
    "\n",
    "#Extract the title \n",
    "name=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in name[0:7]:\n",
    "    Title.append(i.text)\n",
    "#Extract the Ratings\n",
    "R=driver.find_elements(By.XPATH,'//span[@class=\"a-size-base\"]')\n",
    "for i in R[0:7]:\n",
    "    Ratings.append(i.text)\n",
    "#Extract the Price\n",
    "Pr=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in Pr[0:7]:\n",
    "    Price.append(i.text)\n",
    "    \n",
    "#create the dataframe\n",
    "\n",
    "df=pd.DataFrame({'Title':Title,'Ratings':Ratings,'Price':Price})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89bffe",
   "metadata": {},
   "source": [
    "Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the url link under url variable\n",
    "url='https://www.azquotes.com/'\n",
    "#load chrome driver\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "driver.get(url)\n",
    "# maximise the window\n",
    "driver.maximize_window()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24072f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the top quotes tab\n",
    "top_quotes=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a')\n",
    "top_quotes.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list to store collecting values\n",
    "Quotes=[]\n",
    "Author=[]\n",
    "Type_Of_Quotes=[]\n",
    "\n",
    "# mention the page numbers\n",
    "start=0\n",
    "end=9\n",
    "\n",
    "for j in range(start,end):\n",
    "    #collect quote,auther and tyoe of quote\n",
    "    Quote=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "    auth=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "    type_quote=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "    # store elements 1 by 1 into respective list\n",
    "    for i in  Quote:\n",
    "        Quotes.append(i.text)\n",
    "    for j in auther:\n",
    "        Author.append(j.text)\n",
    "    for k in type_quote:\n",
    "        Type_Of_Quotes.append(k.text)\n",
    "    # apply the action on next button    \n",
    "    next_button=driver.find_element(By.XPATH,'//li[@class=\"next\"]')\n",
    "    next_button.click()\n",
    "    \n",
    "# create the dataframe\n",
    "\n",
    "df=pd.DataFrame({'Quote':Quotes,'Author':Author,'Type_Of_Quotes':Type_Of_Quotes})\n",
    "df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f4b55",
   "metadata": {},
   "source": [
    "Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, \n",
    "Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make theDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "1817ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the url link under url variable\n",
    "url='https://www.jagranjosh.com/'\n",
    "#load chrome driver\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "driver.get(url)\n",
    "# maximise the window\n",
    "driver.maximize_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "64e673a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the gk option\n",
    "gk=driver.find_element(By.XPATH,'//*[@id=\"1540978020504\"]/div[1]/header/div[3]/ul/li[9]/a')\n",
    "gk.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "56776472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the popup\n",
    "# to avoid the Ad and notification popup\n",
    "import pyautogui\n",
    "pyautogui.press('esc')\n",
    "#close_Add=driver.find_element(By.XPATH,'//span[@class=\"ns-4wtf3-e-21\"]')\n",
    "#close_Add.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "584a4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the list of prime ministers\n",
    "pm=driver.find_element(By.XPATH,'//*[@id=\"popluarGK\"]/ul/li[2]/a')\n",
    "pm.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "ce4b363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign row number\n",
    "r = 2\n",
    "templist = []\n",
    " \n",
    "while(1):\n",
    "    try:\n",
    "        name = driver.find_element(By.XPATH, '//*[@id=\"itemdiv\"]/div[5]/span/div[2]/table/tbody/tr['+str(r)+']/td[2]').text\n",
    "        born_and_dead=driver.find_element(By.XPATH,'//*[@id=\"itemdiv\"]/div[5]/span/div[2]/table/tbody/tr['+str(r)+']/td[3]').text\n",
    "        Term_of_office=driver.find_element(By.XPATH,'//*[@id=\"itemdiv\"]/div[5]/span/div[2]/table/tbody/tr['+str(r)+']/td[4]').text\n",
    "        Remark=driver.find_element(By.XPATH,'//*[@id=\"itemdiv\"]/div[5]/span/div[2]/table/tbody/tr['+str(r)+']/td[5]').text\n",
    "        Table_dict = {'Name': name,'born-dead':born_and_dead,'Term_of_office':Term_of_office,\n",
    "                     'Remark':Remark}\n",
    "                      \n",
    " # store in list\n",
    "        templist.append(Table_dict)\n",
    " # create the df       \n",
    "        df = pd.DataFrame(templist)\n",
    " # iterate the row number\n",
    "        r  =r+ 1\n",
    "    except NoSuchElementException:\n",
    "        break\n",
    "        \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c3b64",
   "metadata": {},
   "source": [
    "Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e. \n",
    "Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.motor1.com/\n",
    "2. Then You have to click on the List option from Dropdown menu on leftside.\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "335643b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.motor1.com/'\n",
    "#load chrome driver\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "driver.get(url)\n",
    "# maximise the window\n",
    "driver.maximize_window()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb0f5cf",
   "metadata": {},
   "source": [
    "# There is no '50 most expensive cars in the world' in the website hence i am not able do the scrapping.\n",
    "\n",
    "please let me know what to do in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369a250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
