{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import selenium \n",
    "from selenium import webdriver \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16441f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90dfed7",
   "metadata": {},
   "source": [
    "#1. Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the chrome dirver\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "# open amazon.in wbesite \n",
    "driver.get('https://www.amazon.in/')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# enter the keyword \n",
    "print('Enter Your search Keyword:')\n",
    "search_keyword=input()\n",
    "# find search bar element to pass the keyword\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search.send_keys(search_keyword)\n",
    "\n",
    "\n",
    "# click on the search button\n",
    "search_click=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search_click.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad8905",
   "metadata": {},
   "source": [
    "#2. In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the chrome dirver\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "# open amazon.in wbesite \n",
    "driver.get('https://www.amazon.in/')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# enter the keyword \n",
    "print('Enter Your search Keyword:')\n",
    "search_keyword=input()\n",
    "# find search bar element to pass the keyword\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search.send_keys(search_keyword)\n",
    "\n",
    "\n",
    "# click on the search button\n",
    "search_click=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search_click.click()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=2\n",
    "product_link=[]\n",
    "\n",
    "for i in range(start,end):\n",
    "    url1=driver.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')   \n",
    "    #url1=driver.find_elements(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[1]/div[1]/div[3]/div/div/div/div/div/div/div[2]/div[1]/h2/a')\n",
    "    for j in url1:\n",
    "        link=j.get_attribute('href')\n",
    "        product_link.append(link)\n",
    "        \n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Brand_Name=[]\n",
    "Price=[]\n",
    "Return_Exchange=[]\n",
    "Expected_Deliver=[]\n",
    "Name_of_the_Product=[]\n",
    "Availability=[]\n",
    "Product_URL=[]\n",
    "\n",
    "\n",
    "for i in product_link:\n",
    "    driver.get(i)\n",
    "    # collect the product title\n",
    "    try:\n",
    "        name=driver.find_element(By.XPATH,'//span[@class=\"a-size-large product-title-word-break\"]')\n",
    "        Name_of_the_Product.append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        Name_of_the_Product.append('-')\n",
    "        \n",
    "    # collect the product brand name\n",
    "    try:\n",
    "        b_name=driver.find_element(By.XPATH,'//a[@id=\"bylineInfo\"]')\n",
    "        Brand_Name.append(b_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Brand_Name.append('-')\n",
    "        \n",
    "    # collect the Return_Exchange\n",
    "    try:\n",
    "        return_policy=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[6]/div[3]/div[4]/div[22]/div[2]/div/div/div/div[2]/div/ol/li[3]/div/span/div[2]')\n",
    "        Return_Exchange.append(return_policy.text)\n",
    "    except NoSuchElementException:\n",
    "        Return_Exchange.append('-')\n",
    "    \n",
    "    # collect the delivery date information\n",
    "    try:\n",
    "        delivery=driver.find_element(By.XPATH,'//span[@class=\"a-text-bold\"]')\n",
    "        Expected_Deliver.append(delivery.text)\n",
    "    except NoSuchElementException:\n",
    "        Expected_Deliver.append('-')\n",
    "        \n",
    "    # collect the Price details\n",
    "    try:\n",
    "        p_price=driver.find_element(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "        Price.append(p_price.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append('-')\n",
    "        \n",
    "        # collect the Availability details\n",
    "    try:\n",
    "        avail=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[6]/div[3]/div[1]/div[3]/div/div[1]/div/div/div/form/div/div/div/div/div[3]/div/div[4]/div/div[1]/span')\n",
    "        Availability.append(avail.text)\n",
    "    except NoSuchElementException:\n",
    "        Availability.append('-')\n",
    "        \n",
    "          # append product url\n",
    "    try:\n",
    "        Product_URL.append(i)\n",
    "    except NoSuchElementException:\n",
    "        Product_URL.append('-')\n",
    "        \n",
    " # create the data frame\n",
    "df=pd.DataFrame({'Brand_Name':Brand_Name,'Price':Price,'Return_Exchange':Return_Exchange,'Expected_Deliver':Expected_Deliver\n",
    "                 ,'Name_of_the_Product':Name_of_the_Product,'Availability':Availability,'Product_URL':Product_URL})\n",
    "df  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c487ff",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating  list of key words to pass\n",
    "input_lst=['fruits', 'cars','Machine Learning','Guitar', 'Cakes']\n",
    "\n",
    "img_url=[]\n",
    "img_data=[]\n",
    "\n",
    "for l in input_lst:\n",
    "    # load the chrome dirver\n",
    "    driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "    # open googlepage wbesite \n",
    "    driver.get('https://images.google.com/')\n",
    "    driver.maximize_window()\n",
    "    time.sleep(5)\n",
    "\n",
    "    # find the search bar element to pass the search keyword\n",
    "    search=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/textarea')\n",
    "    search.send_keys(l)\n",
    "    # click on the search button\n",
    "    click=driver.find_element(By.XPATH,'//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "    click.click()\n",
    "    for _ in range(20):\n",
    "        driver.execute_script(\"window.scrollBy(0,50)\")\n",
    "    #create the empty list to store url\n",
    "    img_url=[]\n",
    "    images=driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "    # get the image source\n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        # append the extracted url from image source to empty img_url list\n",
    "        if source is not None:\n",
    "            if(source[0:4]=='http'):\n",
    "                img_url.append(source)\n",
    "\n",
    "     # download only 10 images from each keyword by setting up the if condtion   \n",
    "    for i in range(len(img_url)):\n",
    "        if i > 10:\n",
    "            break\n",
    "        print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "        response=requests.get(img_url[i])\n",
    "    # set local system path to store the images\n",
    "        file=open(r\"C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\Webscrapping\\\\images download\\\\\"+str(l)+str(i)+\".jpg\",\"wb\")\n",
    "        file.write(response.content)\n",
    "    # close the brower once done with 1 search word        \n",
    "    driver.close()\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86343a1c",
   "metadata": {},
   "source": [
    "4.Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cdf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the chrome dirver\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "# open flipkart  wbesite \n",
    "driver.get('https://www.flipkart.com')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# enter the keyword \n",
    "print('Enter Your search Keyword:')\n",
    "search_keyword=input()\n",
    "# find search bar element to pass the keyword\n",
    "search=driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]')\n",
    "search.send_keys(search_keyword)\n",
    "\n",
    "loginpage=driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _2doB4z\"]')\n",
    "loginpage.click()\n",
    "\n",
    "\n",
    "# click on the search button\n",
    "search_click=driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]')\n",
    "search_click.click()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exract the product page url\n",
    "url=[]\n",
    "\n",
    "link=driver.find_elements(By.XPATH,'//a[@class=\"_1fQZEK\"]')\n",
    "\n",
    "for i in link:\n",
    "    page=i.get_attribute('href')\n",
    "    url.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47317d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list for every attribute \n",
    "Brand_name=[]\n",
    "Smartphone_name=[]\n",
    "Colour=[]\n",
    "RAM=[] \n",
    "Storage_ROM=[]\n",
    "Primary_Camera=[]\n",
    "Secondary_Camera=[]\n",
    "Display_Size=[]\n",
    "Battery_Capacity=[]\n",
    "Price=[] \n",
    "Product_URL=[]\n",
    "\n",
    "for i in url:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # expand the more details page\n",
    "    more_button=driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _1FH0tX\"]')\n",
    "    more_button.click()\n",
    "\n",
    "    try:\n",
    "        Title=driver.find_element(By.XPATH,'//span[@class=\"B_NuCI\"]')\n",
    "        Smartphone_name.append(Title.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphone_name.append('-')\n",
    "        \n",
    "## scrap brand name details          \n",
    "    try:\n",
    "        Title=driver.find_element(By.XPATH,'//span[@class=\"B_NuCI\"]')\n",
    "        name=Title.text.split(\" \")[0:1]\n",
    "        Brand_name.append(name)\n",
    "    except NoSuchElementException:\n",
    "        Brand_name.append('-')     \n",
    "        \n",
    "        \n",
    "## scrap Color details          \n",
    "    try:\n",
    "        clr=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[5]/div/div[2]/div[1]/div[1]/table/tbody/tr[4]')\n",
    "        t=clr.text.split('\\n')[1]\n",
    "        Colour.append(t)\n",
    "    except NoSuchElementException:\n",
    "        Colour.append('-')\n",
    "        \n",
    "# scrap RAM details   \n",
    "    try:\n",
    "        ram_memory=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div/div/div[5]/div/div[2]/div[1]/div[4]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        RAM.append(ram_memory.text)\n",
    "    except NoSuchElementException:\n",
    "        RAM.append('-')\n",
    "# scrap Storage_ROM details        \n",
    "    try:    \n",
    "        ROM=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div/div/div[5]/div/div[2]/div[1]/div[4]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Storage_ROM.append(ROM.text)\n",
    "    except NoSuchElementException:\n",
    "        Storage_ROM.append('-')\n",
    "# scrap Primary_Camera details    \n",
    "    try:\n",
    "        main_camera=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div/div/div[5]/div/div[2]/div[1]/div[5]/table/tbody/tr/td[2]/ul/li')\n",
    "        Primary_Camera.append(main_camera.text)\n",
    "    except NoSuchElementException:\n",
    "        Primary_Camera.append('-')\n",
    "# scrap Secondary_Camera details\n",
    "    try:\n",
    "        front_Camera=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div[5]/div/div[2]/div[1]/div[5]/table/tbody/tr[6]/td[2]/ul/li')\n",
    "        Secondary_Camera.append(front_Camera.text)\n",
    "    except NoSuchElementException:\n",
    "        Secondary_Camera.append('-')\n",
    "    \n",
    "# scrap display size \n",
    "    try:\n",
    "        display=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[7]/div/div/div[4]/div/div[2]/div[1]/div[2]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Display_Size.append(display.text)\n",
    "    except NoSuchElementException:\n",
    "        Display_Size.append('-')\n",
    "\n",
    "        # scrap battery info  \n",
    "    try: \n",
    "        battery=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[5]/div/div[2]/div[1]/div[7]/table/tbody/tr/td[2]/ul/li')\n",
    "        Battery_Capacity.append(battery.text)\n",
    "    except:\n",
    "        Battery_Capacity.append('-')\n",
    "        \n",
    "        # Price    \n",
    "    try: \n",
    "        pr=driver.find_element(By.XPATH,'//div[@class=\"_30jeq3 _16Jk6d\"]')\n",
    "        Price.append(pr.text)\n",
    "    except:\n",
    "        Price.append('-')\n",
    "        \n",
    "        # scrap battery info \n",
    "              \n",
    "    try: \n",
    "        \n",
    "        Product_URL.append(i)\n",
    "    except:\n",
    "        Battery_Capacity.append('-')\n",
    "        \n",
    "        \n",
    " # cerate the datafrae\n",
    "\n",
    "\n",
    "# cerate the datafrae\n",
    "\n",
    "df=pd.DataFrame({'Smartphone_name':Smartphone_name,'Colour':Colour,'RAM':RAM,'Storage_ROM':Storage_ROM,\n",
    "                 'Primary_Camera':Primary_Camera,'Secondary_Camera':Secondary_Camera,'Display_Size':Display_Size,\n",
    "                 'Battery_Capacity':Battery_Capacity,'Price':Price,'Product_URL':Product_URL})\n",
    "\n",
    "# save it as csv \n",
    "df.to_csv('oneplus_nord.csv')\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc00f2",
   "metadata": {},
   "source": [
    "5.Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74428fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the chrome dirver\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "# open google  wbesite \n",
    "driver.get('https://www.google.com/maps/')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "# find the search bar to send the input\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/form/div[2]/div[3]/div/input[1]\")\n",
    "print('Enter you city Name')\n",
    "search.send_keys(input())\n",
    "\n",
    "\n",
    "# click on the search button\n",
    "click_button=driver.find_element(By.XPATH,'//div[@class=\"pzfvzf\"]')\n",
    "click_button.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extarct the lat and long \n",
    "lat=[]\n",
    "long=[]\n",
    "try:\n",
    "    url=driver.current_url\n",
    "    print('The current url is:',url)\n",
    "    lat_long=re.findall(r'@(.*)data',url)\n",
    "    for i in lat_long:\n",
    "        t=i.split(\",\")\n",
    "    \n",
    "        lat.append(t[0])\n",
    "        long.append(t[1])\n",
    "except:\n",
    "        lat.append('-')\n",
    "        long.append('-')    \n",
    "\n",
    "print(lat)\n",
    "print(long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf8815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d01fc",
   "metadata": {},
   "source": [
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd46ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the chrome dirver\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "delay=5\n",
    "try:\n",
    "    # open digit.in wbesite \n",
    "    driver.get('https://digit.in/')\n",
    "    driver.maximize_window()\n",
    "    # lets load the complete page to ensure of\n",
    "    WebDriverWait(driver,delay).until(ec.presence_of_element_located((By.CLASS_NAME,'list-social list-inline')))\n",
    "except TimeoutException as t:\n",
    "    print(t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # find the best laptop element\n",
    "bestlap=driver.find_element(By.XPATH,'/html/body/div[7]/div/div[2]/div[2]/div[3]/ul/li[9]/a')\n",
    "bestlap.click()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f67406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat the empty list to store values\n",
    "s_no=[]\n",
    "lap_name=[]\n",
    "Description=[]\n",
    "processor=[]\n",
    "Display=[]\n",
    "OS=[]\n",
    "Memory=[]\n",
    "Graphics_Processor=[]\n",
    "Body=[]\n",
    "\n",
    "# collect name and serial number\n",
    "try:\n",
    "    laptops=driver.find_elements(By.XPATH,'//div[@class=\"TopNumbeHeading\"]')\n",
    "    for i in laptops:\n",
    "        t=i.text.split('\\n')\n",
    "        s_no.append(t[0])\n",
    "        lap_name.append(t[1])\n",
    "except:\n",
    "    s_no.append('-')\n",
    "    name.append('-')\n",
    "        \n",
    "# collect description\n",
    "try:\n",
    "    desc=driver.find_elements(By.XPATH,'//div[@class=\"tptn-prod-desc\"]')\n",
    "    for i in desc:\n",
    "        Description.append(i.text)\n",
    "except:\n",
    "    Description.append('-')\n",
    "    \n",
    "# extract data from table \n",
    "x=driver.find_elements(By.XPATH,'//div[@class=\"Spcs-details\"]')\n",
    "try:\n",
    "    for i in x:\n",
    "        data=i.text.split('\\n')\n",
    "        name=data[1].split(':')\n",
    "        processor.append(name[1])\n",
    "        name1=data[2].split(':')\n",
    "        Display.append(name1[1])\n",
    "        name2=data[3].split(':')\n",
    "        OS.append(name2[1])\n",
    "        name3=data[4].split(':')\n",
    "        Memory.append(name3[1])\n",
    "        name4=data[5].split(':')\n",
    "        Graphics_Processor.append(name4[1])\n",
    "        name5=data[6].split(':')\n",
    "        Body.append(name5[1])\n",
    "except:\n",
    "    processor.append('-')\n",
    "    Display.append('-')\n",
    "    OS.append('-')\n",
    "    Memory.append('-')\n",
    "    Graphics_Processor.append('-')\n",
    "    Body.append('-')\n",
    "    \n",
    "# create the data frame\n",
    "df=pd.DataFrame({'No':s_no,'laptop_Name':lap_name,'Description':Description,\n",
    "                 'processor':processor,'Display':Display,'OS':OS,'Memory':Memory,\n",
    "                 'Graphics_Processor':Graphics_Processor,'Body':Body})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7ee38",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "#open digit.in wbesite \n",
    "driver.get('https://www.forbes.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "# click on the menu list on the page\n",
    "menu=driver.find_element(By.XPATH,'//div[@class=\"_69hVhdY4\"]')\n",
    "menu.click()\n",
    "\n",
    "# click on the billionaries tab\n",
    "direction=driver.find_element(By.XPATH,'//div[@class=\"mpBfVZz3\"]')\n",
    "direction.click()\n",
    "\n",
    "\n",
    "# click on the top billionaries tab\n",
    "top_billionaries=driver.find_element(By.XPATH,'//li[@class=\"TjJgrPSg _2bNo56RE secondary\"]')\n",
    "top_billionaries.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0793e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "# collect all the element from the page respect to billioniries table\n",
    "d=driver.find_elements(By.XPATH,'//div[@class=\"TableRow_cell__db-hv Table_cell__houv9\"]')\n",
    "for i in d:\n",
    "    data.append(i.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to split the text by 7 to get every row data \n",
    "n = 7\n",
    "# splite the data \n",
    "final_list = [data[i:i+n] for i in range(0, len(data), n)]\n",
    "# create the empty list to store values  \n",
    "Rank=[]\n",
    "Name=[]\n",
    "Networth=[]\n",
    "Age=[]\n",
    "country=[]\n",
    "source=[]\n",
    "INDUSTRY=[]\n",
    "\n",
    "for i in range(0,len(final_list)):\n",
    "    data=final_list[i]\n",
    "    for j in range(0,1):\n",
    "        Rank.append(data[0])\n",
    "        Name.append(data[1])\n",
    "        Networth.append(data[2])\n",
    "        Age.append(data[3])\n",
    "        country.append(data[4])\n",
    "        source.append(data[5])\n",
    "        INDUSTRY.append(data[6])\n",
    "\n",
    "    \n",
    " # create the dataframe\n",
    "df=pd.DataFrame({'Rank':Rank,'Name':Name,'Networth':Networth,'Age':Age,\n",
    "               'country':country,'source':source,'INDUSTRY':INDUSTRY})\n",
    "df        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436299fe",
   "metadata": {},
   "source": [
    "#8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "#open digit.in wbesite \n",
    "driver.get('https://www.youtube.com/watch?v=jtn-hRJjl68')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "# scroll by 1000 \n",
    "for i in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,500)\")\n",
    "    #time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefdb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap the comments:\n",
    "c=[]\n",
    "comments=driver.find_elements(By.XPATH,'//span[@class=\"style-scope yt-formatted-string\"]')\n",
    "for i in comments[0:500]:\n",
    "    c.append(i.text)\n",
    "    \n",
    "# extract the date of comment\n",
    "d=[]\n",
    "date=driver.find_elements(By.XPATH,'//a[@class=\"yt-simple-endpoint style-scope yt-formatted-string\"]')\n",
    "for i in date[0:500]:\n",
    "    d.append(i.text)\n",
    "    \n",
    "# create the dataframe\n",
    "df=pd.DataFrame({'comment':c,'date_of_comment':d})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23773e28",
   "metadata": {},
   "source": [
    "9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "id": "5a2173aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('C:\\\\Users\\\\Hp\\\\OneDrive\\\\Desktop\\\\intership\\\\chromedriver_win32 (1)\\\\chromedriver.exe')\n",
    "#open www.hostelworld.com wbesite \n",
    "driver.get('https://www.hostelworld.com/')\n",
    "driver.maximize_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "id": "64ee405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find search bar and enter the location \n",
    "search=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div/div/div[4]/div/div[2]/div/div[1]/div/div/div/input')\n",
    "search.send_keys('London')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "8b27a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on the search button\n",
    "result=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div/div/div[4]/div/div[2]/div/div[1]/div/div/ul/li[2]')\n",
    "result.click()\n",
    "\n",
    "search_btn=driver.find_element(By.XPATH,'//div[@class=\"search-button\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "fca06186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel</th>\n",
       "      <th>distance_from_city_centre</th>\n",
       "      <th>Rating</th>\n",
       "      <th>privates_from_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [hotel, distance_from_city_centre, Rating, privates_from_price]\n",
       "Index: []"
      ]
     },
     "execution_count": 974,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel=[]\n",
    "distance_from_city_centre=[]\n",
    "Rating=[]\n",
    "privates_from_price=[]\n",
    "\n",
    "try:\n",
    "    h_name=driver.find_elements(By.XPATH,'//h2[@class=\"title title-6\"]')\n",
    "\n",
    "    for i in h_name:\n",
    "        hotel.append(i.text)\n",
    "except:\n",
    "    hotel.append('-')\n",
    "    \n",
    "try:\n",
    "    distance=driver.find_elements(By.XPATH,'//span[@class=\"description\"]')\n",
    "    for i in distance:\n",
    "        distance_from_city_centre.append(i.text.split('-')[1])\n",
    "except:\n",
    "    distance_from_city_centre.append('-')\n",
    "    \n",
    "try:\n",
    "    ratings=driver.find_elements(By.XPATH,'//div[@class=\"score orange big\"]')\n",
    "    for i in ratings:\n",
    "        Rating.append(i.text)\n",
    "except:\n",
    "    Rating.append('-')\n",
    "\n",
    "try:\n",
    "    p_prce=driver.find_elements(By.XPATH,'//div[@class=\"price title-5\"]')\n",
    "    for i in p_prce:\n",
    "        privates_from_price.append(i.text)\n",
    "except:\n",
    "    privates_from_price.append('-')\n",
    "\n",
    "    \n",
    " # create the df\n",
    "\n",
    "df=pd.DataFrame({'hotel':hotel,'distance_from_city_centre':distance_from_city_centre,'Rating':\n",
    "                Rating,'privates_from_price':privates_from_price})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e45ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
